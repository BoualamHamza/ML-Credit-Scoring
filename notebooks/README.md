# Guide d'utilisation MLFlow dans les Notebooks

## ğŸš€ DÃ©marrage Rapide

### 1. Configuration dans votre notebook

```python
# Import de l'utilitaire
from src.utils.notebook_mlflow import quick_mlflow_setup

# Initialiser MLFlow
mlflow_tracker = quick_mlflow_setup("mon-experience")
```

### 2. EntraÃ®ner et logger un modÃ¨le

```python
from sklearn.ensemble import RandomForestClassifier

# CrÃ©er le modÃ¨le
model = RandomForestClassifier(n_estimators=100, random_state=42)

# EntraÃ®ner et logger automatiquement
metrics = mlflow_tracker.log_experiment(
    model=model,
    model_name="RandomForest",
    X_train=X_train, X_test=X_test, 
    y_train=y_train, y_test=y_test,
    params={"n_estimators": 100, "random_state": 42},
    tags={"baseline": "true"}
)
```

### 3. Comparer plusieurs modÃ¨les

```python
models_config = [
    {
        "model": LogisticRegression(random_state=42),
        "name": "LogisticRegression",
        "params": {"random_state": 42}
    },
    {
        "model": GradientBoostingClassifier(n_estimators=100),
        "name": "GradientBoosting", 
        "params": {"n_estimators": 100}
    }
]

# Comparer tous les modÃ¨les
results = mlflow_tracker.compare_models(
    models_config, X_train, X_test, y_train, y_test
)
```

## ğŸ“Š FonctionnalitÃ©s Automatiques

### MÃ©triques TrackÃ©es
- âœ… **Accuracy**: PrÃ©cision globale
- âœ… **Precision**: PrÃ©cision
- âœ… **Recall**: Rappel  
- âœ… **F1-Score**: Score F1
- âœ… **AUC-ROC**: CapacitÃ© de discrimination
- âœ… **Business Cost**: CoÃ»t mÃ©tier (FN = 10x FP)
- âœ… **Optimal Threshold**: Seuil optimisÃ©

### Visualisations Automatiques
- ğŸ“ˆ Matrice de confusion
- ğŸ“Š Distribution des probabilitÃ©s
- ğŸ“ˆ Courbe ROC
- ğŸ“ˆ Courbe Precision-Recall

### Informations sur les DonnÃ©es
- ğŸ“Š Nombre d'Ã©chantillons train/test
- ğŸ¯ Taux de dÃ©faut
- âš–ï¸ Ratio de dÃ©sÃ©quilibre des classes
- ğŸ”¢ Nombre de features

## ğŸ¯ Exemples d'Utilisation

### Exemple 1: ModÃ¨le Simple
```python
# Configuration
mlflow_tracker = quick_mlflow_setup("credit-scoring")

# ModÃ¨le
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100, random_state=42)

# EntraÃ®nement et logging
metrics = mlflow_tracker.log_experiment(
    model=model,
    model_name="RandomForest",
    X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test
)
```

### Exemple 2: Comparaison de ModÃ¨les
```python
# Liste des modÃ¨les Ã  comparer
models_config = [
    {
        "model": LogisticRegression(random_state=42),
        "name": "LogisticRegression"
    },
    {
        "model": RandomForestClassifier(n_estimators=100, random_state=42),
        "name": "RandomForest"
    },
    {
        "model": GradientBoostingClassifier(n_estimators=100, random_state=42),
        "name": "GradientBoosting"
    }
]

# Comparaison automatique
results = mlflow_tracker.compare_models(
    models_config, X_train, X_test, y_train, y_test
)
```

### Exemple 3: Hyperparameter Tuning
```python
from sklearn.model_selection import GridSearchCV

# Configuration de la grille
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15]
}

# Grid Search
grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid, cv=5, scoring='roc_auc'
)

# EntraÃ®nement
grid_search.fit(X_train, y_train)

# Logging du meilleur modÃ¨le
metrics = mlflow_tracker.log_experiment(
    model=grid_search.best_estimator_,
    model_name="RandomForest_GridSearch",
    X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test,
    params=grid_search.best_params_,
    tags={"hyperparameter_tuning": "true"}
)
```

## ğŸŒ Interface MLFlow

### AccÃ¨s Ã  l'Interface
1. **DÃ©marrer le serveur MLFlow**:
   ```bash
   source .venv/bin/activate && mlflow ui
   ```

2. **Ouvrir dans le navigateur**: http://localhost:5000

### FonctionnalitÃ©s de l'Interface
- ğŸ“Š **ExpÃ©rimentations**: Voir toutes vos expÃ©rimentations
- ğŸ” **Runs**: DÃ©tails de chaque run avec mÃ©triques
- ğŸ“ˆ **Comparaison**: Comparer les modÃ¨les cÃ´te Ã  cÃ´te
- ğŸ“¦ **ModÃ¨les**: Registry des modÃ¨les entraÃ®nÃ©s
- ğŸ“Š **Graphiques**: Visualiser les courbes et matrices

## ğŸ¯ Bonnes Pratiques

### 1. Nommage des ExpÃ©rimentations
```python
# Utilisez des noms descriptifs
mlflow_tracker = quick_mlflow_setup("credit-scoring-feature-engineering")
mlflow_tracker = quick_mlflow_setup("credit-scoring-hyperparameter-tuning")
```

### 2. Tags Informatifs
```python
tags = {
    "baseline": "true",
    "feature_engineering": "none",
    "data_version": "v1.0",
    "business_context": "credit-scoring"
}
```

### 3. ParamÃ¨tres DÃ©taillÃ©s
```python
params = {
    "model_type": "RandomForest",
    "n_estimators": 100,
    "max_depth": 10,
    "random_state": 42,
    "preprocessing": "StandardScaler",
    "feature_selection": "None"
}
```

## ğŸ”§ Configuration AvancÃ©e

### URI de Tracking PersonnalisÃ©e
```python
from src.utils.notebook_mlflow import NotebookMLFlow

# Configuration personnalisÃ©e
mlflow_tracker = NotebookMLFlow(
    experiment_name="mon-experience",
    tracking_uri="sqlite:///custom/path/mlflow.db"
)
```

### RÃ©cupÃ©ration du Meilleur ModÃ¨le
```python
# RÃ©cupÃ©rer le meilleur modÃ¨le selon le coÃ»t mÃ©tier
best_model = mlflow_tracker.get_best_model("business_cost", ascending=True)
print(f"Meilleur modÃ¨le: {best_model['run_id']}")
print(f"CoÃ»t mÃ©tier: {best_model['metric_value']}")
```

## ğŸ†˜ DÃ©pannage

### ProblÃ¨mes Courants
1. **Import Error**: VÃ©rifiez que vous Ãªtes dans le bon rÃ©pertoire
2. **MLFlow URI**: Assurez-vous que le chemin vers la base de donnÃ©es est correct
3. **Permissions**: VÃ©rifiez les permissions sur les dossiers `mlruns/` et `mlartifacts/`

### Logs et Debug
```python
# Activer les logs MLFlow
import logging
logging.basicConfig(level=logging.INFO)
```

## ğŸ“š Ressources

- [Documentation MLFlow](https://mlflow.org/docs/latest/index.html)
- [MLFlow Tracking](https://mlflow.org/docs/latest/tracking.html)
- [MLFlow Models](https://mlflow.org/docs/latest/models.html)



